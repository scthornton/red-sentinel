# RedSentinel ğŸš¨

**RedSentinel** is a comprehensive red team tool for testing LLM guardrails and detecting system prompt extraction.

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip3 install -r requirements.txt
```

### 2. Test Installation
```bash
python3 test_installation.py
```

### 3. Run Demo
```bash
python3 demo.py
```

### 4. Generate Training Data
```bash
python3 scripts/generate_training_data.py
```

### 5. Train ML Models
```bash
python3 scripts/run_ml_pipeline.py --data training_data/synthetic_attacks.csv
```

## ğŸ“ Project Structure

```
red-one/
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ core/                     # Attack logging & evaluation
â”‚   â”œâ”€â”€ features/                 # Feature extraction
â”‚   â””â”€â”€ ml/                       # Machine learning pipeline
â”œâ”€â”€ config/                       # Configuration files
â”œâ”€â”€ scripts/                      # Utility scripts
â”œâ”€â”€ demo.py                       # Complete demo
â”œâ”€â”€ test_installation.py          # Installation test
â””â”€â”€ requirements.txt              # Dependencies
```

## ğŸ”§ Core Components

- **AttackLogger**: Logs multi-step attacks with automatic evaluation
- **PromptEvaluator**: Detects system prompt extraction and refusal patterns
- **FeatureExtractor**: Converts logs to ML-ready datasets
- **MLPipeline**: Trains models to predict attack success

## ğŸ“Š Features

- âœ… Multi-step attack tracking
- âœ… Automatic response evaluation
- âœ… Feature engineering pipeline
- âœ… Multiple ML algorithms
- âœ… Cross-validation & reporting
- âœ… CSV/JSON logging

## ğŸ¯ Use Cases

- Red team operations
- LLM security testing
- Guardrail validation
- Attack pattern analysis
- Security research

---

**RedSentinel** - Protecting AI systems through intelligent red team testing ğŸ›¡ï¸
