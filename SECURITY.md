# Security Policy

## Responsible Use

**RedSentinel** is a red team tool for authorized security testing of LLM systems and guardrails.

### Authorized Use

✅ **Permitted:**
- Red team operations on systems you own or have explicit permission to test
- LLM security testing in authorized lab environments
- Guardrail validation for your own AI systems
- Attack pattern analysis for defensive purposes
- Security research in controlled environments
- Educational purposes with proper authorization

### Prohibited Use

❌ **Not Permitted:**
- Unauthorized testing of production LLM systems
- Attacking third-party AI services without permission
- Bypassing security controls without authorization
- Any activity violating terms of service
- Malicious use or weaponization

## Reporting a Vulnerability

If you discover a security vulnerability in RedSentinel:

**Email:** scott@perfecxion.ai

Please include:
- Description of the vulnerability
- Steps to reproduce
- Potential impact
- Suggested fixes

### Response Timeline

- **Initial Response:** Within 48 hours
- **Assessment:** Within 7 days
- **Resolution:** Based on severity

## Supported Versions

| Version | Supported          |
| ------- | ------------------ |
| main    | :white_check_mark: |

## Best Practices

When using RedSentinel:

1. **Authorization First** - Always obtain explicit permission before testing
2. **Controlled Environment** - Use isolated test systems
3. **Document Testing** - Maintain logs and audit trails
4. **Responsible Disclosure** - Report findings appropriately
5. **Defensive Focus** - Prioritize building better defenses

## Ethical Guidelines

This tool is designed for defensive security purposes:
- Understanding attack patterns to build better protections
- Validating security controls before deployment
- Improving AI system resilience
- Advancing security research responsibly

## Contact

- **Email:** scott@perfecxion.ai
- **Alternative:** scthornton@gmail.com

For security concerns or responsible use questions, contact scott@perfecxion.ai.
